# Метод Гаусса-Зейделя для решения систем линейных уравнений

## Теоретическое введение

Метод Гаусса-Зейделя – это итерационный метод решения систем линейных алгебраических уравнений (СЛАУ). Он относится к классу методов последовательных приближений и является улучшением метода простой итерации (метода Якоби).

Рассмотрим систему линейных уравнений в матричной форме:

```
Ax = b
```

где:
- A - матрица коэффициентов размера n×n
- x - вектор неизвестных размера n
- b - вектор правой части размера n

## Алгоритм метода Гаусса-Зейделя

Основная идея метода Гаусса-Зейделя заключается в последовательном уточнении компонент вектора решения с использованием уже обновленных значений.

### Представление матрицы A

Представим матрицу A в виде суммы трех матриц:

```
A = L + D + U
```

где:
- L - строго нижняя треугольная часть A (все элементы ниже главной диагонали)
- D - диагональная часть A
- U - строго верхняя треугольная часть A (все элементы выше главной диагонали)

### Итерационная формула

Итерационная формула метода Гаусса-Зейделя имеет вид:

```
(L + D)x^(k+1) = b - Ux^(k)
```

или в поэлементной форме:

```
x_i^(k+1) = (b_i - Σ_{j=1}^{i-1} a_{ij}x_j^(k+1) - Σ_{j=i+1}^{n} a_{ij}x_j^(k)) / a_{ii}
```

где:
- x_i^(k) - значение i-й компоненты решения на k-й итерации
- a_{ij} - элемент матрицы A в i-й строке и j-м столбце
- b_i - i-й элемент вектора правой части

### Критерий остановки

Итерационный процесс продолжается до достижения заданной точности:

```
||x^(k+1) - x^(k)|| < ε  или  ||Ax^(k+1) - b|| < ε
```

где ε - заданная точность (например, 1e-6).

## Условия сходимости

Метод Гаусса-Зейделя сходится для любого начального приближения, если матрица A удовлетворяет одному из следующих условий:

1. **Диагональное преобладание**: для каждой строки модуль диагонального элемента больше или равен сумме модулей остальных элементов строки:
   ```
   |a_{ii}| ≥ Σ_{j≠i} |a_{ij}| для всех i
   ```
   и хотя бы для одной строки неравенство строгое.

2. **Симметричная положительно определенная матрица**: A = A^T и x^T A x > 0 для всех ненулевых векторов x.

## Вычислительная сложность

- Каждая итерация требует O(n²) операций
- Число итераций зависит от свойств матрицы A и требуемой точности
- Для хорошо обусловленных систем метод сходится быстрее

## Реализация в Julia

Ниже представлена реализация метода Гаусса-Зейделя в Julia:

```julia
"""
    is_diagonally_dominant(A)

Проверяет, является ли матрица A диагонально доминирующей.
"""
function is_diagonally_dominant(A::AbstractMatrix)
    n = size(A, 1)
    
    for i = 1:n
        diagonal_element = abs(A[i, i])
        sum_of_others = sum(abs.(A[i, :]) - diagonal_element)
        
        if diagonal_element < sum_of_others
            return false
        end
    end
    
    return true
end

"""
    gauss_seidel(A, b, x0=zeros(length(b)), tol=1e-6, max_iter=1000)

Решает систему линейных уравнений Ax = b методом Гаусса-Зейделя.
"""
function gauss_seidel(A::AbstractMatrix, b::AbstractVector, 
                     x0::AbstractVector=zeros(length(b)), 
                     tol::Real=1e-6, max_iter::Integer=1000)
    n = length(b)
    
    # Проверки размерностей
    if size(A, 1) != n || size(A, 2) != n
        error("Матрица A должна быть квадратной размера n×n")
    end
    
    if length(x0) != n
        error("Начальное приближение x0 должно иметь длину n")
    end
    
    # Проверка диагонального доминирования
    if !is_diagonally_dominant(A)
        @warn "Матрица не является диагонально доминирующей. Сходимость метода Гаусса-Зейделя не гарантирована."
    end
    
    # Проверка нулевых элементов на диагонали
    for i = 1:n
        if abs(A[i, i]) < eps()
            error("Нулевой элемент на диагонали в позиции ($i, $i)")
        end
    end
    
    # Инициализация
    x = copy(x0)
    residuals = Float64[]
    
    # Итерационный процесс
    for iter = 1:max_iter
        x_prev = copy(x)
        
        # Один шаг метода Гаусса-Зейделя
        for i = 1:n
            # Вычисляем сумму для j < i (используем уже обновленные значения x)
            sum1 = 0.0
            for j = 1:i-1
                sum1 += A[i, j] * x[j]
            end
            
            # Вычисляем сумму для j > i (используем старые значения x)
            sum2 = 0.0
            for j = i+1:n
                sum2 += A[i, j] * x_prev[j]
            end
            
            # Обновляем x[i]
            x[i] = (b[i] - sum1 - sum2) / A[i, i]
        end
        
        # Вычисляем невязку
        residual = norm(A * x - b) / norm(b)
        push!(residuals, residual)
        
        # Проверяем критерий остановки
        if residual < tol
            return x, iter, residuals
        end
    end
    
    @warn "Метод Гаусса-Зейделя не сошелся за $max_iter итераций"
    return x, max_iter, residuals
end

"""
    gauss_seidel_matrix_form(A, b, x0=zeros(length(b)), tol=1e-6, max_iter=1000)

Решает систему линейных уравнений Ax = b методом Гаусса-Зейделя в матричной форме.
"""
function gauss_seidel_matrix_form(A::AbstractMatrix, b::AbstractVector, 
                                 x0::AbstractVector=zeros(length(b)), 
                                 tol::Real=1e-6, max_iter::Integer=1000)
    n = length(b)
    
    # Проверки размерностей
    if size(A, 1) != n || size(A, 2) != n
        error("Матрица A должна быть квадратной размера n×n")
    end
    
    if length(x0) != n
        error("Начальное приближение x0 должно иметь длину n")
    end
    
    # Создаем матрицы L, D и U
    L = zeros(n, n)
    D = zeros(n, n)
    U = zeros(n, n)
    
    for i = 1:n
        for j = 1:n
            if i > j
                L[i, j] = A[i, j]
            elseif i == j
                D[i, j] = A[i, j]
            else  # i < j
                U[i, j] = A[i, j]
            end
        end
    end
    
    # Инициализация
    x = copy(x0)
    residuals = Float64[]
    
    # Итерационный процесс
    for iter = 1:max_iter
        # Один шаг метода Гаусса-Зейделя в матричной форме
        # x = (D + L)^(-1) * (b - U * x)
        x = (D + L) \ (b - U * x)
        
        # Вычисляем невязку
        residual = norm(A * x - b) / norm(b)
        push!(residuals, residual)
        
        # Проверяем критерий остановки
        if residual < tol
            return x, iter, residuals
        end
    end
    
    @warn "Метод Гаусса-Зейделя не сошелся за $max_iter итераций"
    return x, max_iter, residuals
end
```

## Сравнение с другими методами

### Преимущества метода Гаусса-Зейделя

1. **Простота реализации**: Метод легко программируется и не требует сложных вычислительных операций.
2. **Экономия памяти**: Не требуется хранить дополнительные матрицы (в отличие от прямых методов).
3. **Эффективность для разреженных систем**: Для систем с большим количеством нулевых элементов метод может быть очень эффективным.
4. **Быстрее метода Якоби**: За счет использования уже обновленных значений сходимость обычно быстрее, чем у метода Якоби.

### Недостатки метода Гаусса-Зейделя

1. **Условия сходимости**: Метод сходится не для всех матриц.
2. **Медленная сходимость**: Для плохо обусловленных систем сходимость может быть очень медленной.
3. **Зависимость от начального приближения**: Скорость сходимости может сильно зависеть от выбора начального приближения.

## Примеры

### Пример 1: Диагонально доминирующая система

Рассмотрим систему:
```
10x₁ - x₂ + 2x₃ = 6
-x₁ + 11x₂ - x₃ = 25
2x₁ - x₂ + 10x₃ = -11
```

В матричной форме:
```
A = [10 -1 2; -1 11 -1; 2 -1 10]
b = [6, 25, -11]
```

Решение методом Гаусса-Зейделя:

```julia
A = [10.0 -1.0 2.0; -1.0 11.0 -1.0; 2.0 -1.0 10.0]
b = [6.0, 25.0, -11.0]

# Проверка на диагональное доминирование
println("Матрица диагонально доминирующая: ", is_diagonally_dominant(A))

# Решение методом Гаусса-Зейделя
x0 = zeros(3)
x, iterations, residuals = gauss_seidel(A, b, x0)

println("Решение: ", x)
println("Количество итераций: ", iterations)
println("Итоговая невязка: ", residuals[end])

# Проверка результата
println("Невязка: ||Ax - b|| = ", norm(A * x - b))

# Сравнение с точным решением
x_exact = A \ b
println("Точное решение: ", x_exact)
println("Ошибка: ||x - x_exact|| = ", norm(x - x_exact))
```

### Пример 2: Недиагонально доминирующая система

Рассмотрим систему:
```
1x₁ + 2x₂ = 5
3x₁ + 4x₂ = 6
```

В матричной форме:
```
A = [1 2; 3 4]
b = [5, 6]
```

Решение методом Гаусса-Зейделя:

```julia
A_nondom = [1.0 2.0; 3.0 4.0]
b_nondom = [5.0, 6.0]

# Проверка на диагональное доминирование
println("Матрица диагонально доминирующая: ", is_diagonally_dominant(A_nondom))

# Решение методом Гаусса-Зейделя
x0_nondom = zeros(2)
x_nondom, iterations_nondom, residuals_nondom = gauss_seidel(A_nondom, b_nondom, x0_nondom, 1e-6, 100)

println("Решение: ", x_nondom)
println("Количество итераций: ", iterations_nondom)
println("Итоговая невязка: ", residuals_nondom[end])

# Проверка результата
println("Невязка: ||Ax - b|| = ", norm(A_nondom * x_nondom - b_nondom))

# Сравнение с точным решением
x_exact_nondom = A_nondom \ b_nondom
println("Точное решение: ", x_exact_nondom)
println("Ошибка: ||x - x_exact|| = ", norm(x_nondom - x_exact_nondom))
```

## Практические рекомендации

1. **Проверяйте диагональное доминирование** перед использованием метода Гаусса-Зейделя.
2. **Для недиагонально доминирующих матриц** можно попробовать переупорядочить уравнения для повышения вероятности сходимости.
3. **Выбирайте хорошее начальное приближение**, если возможно. Например, решение похожей системы или приближенное решение, полученное другим методом.
4. **Используйте релаксацию** для ускорения сходимости:
   ```
   x_i^(k+1) = ω·x_i^{GS} + (1-ω)·x_i^(k)
   ```
   где x_i^{GS} - значение, полученное обычным методом Гаусса-Зейделя, ω - параметр релаксации (0 < ω < 2).
   При ω > 1 получаем метод верхней релаксации (SOR), который может значительно ускорить сходимость.
5. **Для симметричных положительно определенных матриц** метод Гаусса-Зейделя всегда сходится.
6. **Для больших разреженных систем** метод Гаусса-Зейделя может быть очень эффективным, особенно в сочетании с предобуславливанием.

## Расширения и модификации метода

### 1. Метод последовательной верхней релаксации (SOR)

Метод SOR обобщает метод Гаусса-Зейделя, вводя параметр релаксации ω:

```
x_i^(k+1) = (1-ω)·x_i^(k) + ω·(b_i - Σ_{j<i} a_{ij}x_j^(k+1) - Σ_{j>i} a_{ij}x_j^(k)) / a_{ii}
```

При ω = 1 получаем обычный метод Гаусса-Зейделя. При правильном выборе ω > 1 (обычно в диапазоне 1 < ω < 2) можно значительно ускорить сходимость.

### 2. Блочный метод Гаусса-Зейделя

Для систем с блочной структурой можно применять блочный метод Гаусса-Зейделя, где обновляются целые блоки переменных одновременно. Это может повысить скорость сходимости и удобно для параллельных вычислений.

### 3. Метод Гаусса-Зейделя с предобуславливанием

Предобуславливание улучшает сходимость для плохо обусловленных систем:

```
M^(-1)·A·x = M^(-1)·b
```

где M - предобуславливающая матрица, близкая к A, но более простая для обращения.

## Задания для самостоятельной работы

1. Реализуйте метод SOR (последовательной верхней релаксации) и исследуйте влияние параметра релаксации ω на скорость сходимости.
2. Сравните методы Якоби и Гаусса-Зейделя для различных типов матриц.
3. Исследуйте сходимость метода Гаусса-Зейделя для различных начальных приближений.
4. Реализуйте блочный метод Гаусса-Зейделя для систем специального вида.
5. Исследуйте влияние порядка обработки уравнений на скорость сходимости метода Гаусса-Зейделя.
6. Разработайте параллельную версию метода Гаусса-Зейделя для многоядерных архитектур.
7. Примените метод Гаусса-Зейделя с предобуславливанием для решения плохо обусловленных систем. 