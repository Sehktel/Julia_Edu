# NumericalDifferentiation.jl

Модуль для численного дифференцирования функций в Julia.

## Установка

```julia
using Pkg
Pkg.add(url="https://github.com/yourusername/NumericalDifferentiation.jl")
```

## Основные возможности

Модуль предоставляет следующие возможности:

1. **Базовые методы дифференцирования**:
   - Конечная разность вперед (`forward_difference`)
   - Конечная разность назад (`backward_difference`)
   - Центральная разность (`central_difference`)

2. **Высокоточные методы**:
   - Экстраполяция Ричардсона (`richardson_extrapolation`)
   - Адаптивное дифференцирование (`adaptive_differentiation`)

3. **Производные высших порядков**:
   - Вторые производные (`second_derivative`)
   - Производные произвольного порядка (`higher_derivative`)

4. **Оценка погрешности**:
   - Функции для оценки ошибок численного дифференцирования
   - Выбор оптимального шага дифференцирования

## Примеры использования

### Базовые методы

```julia
using NumericalDifferentiation

# Определение функции
f(x) = sin(x)
x0 = π/4

# Вычисление производной с использованием различных методов
df_forward = forward_difference(f, x0)
df_backward = backward_difference(f, x0)
df_central = central_difference(f, x0)

println("Производная sin(π/4) по методу разности вперед: $df_forward")
println("Производная sin(π/4) по методу разности назад: $df_backward")
println("Производная sin(π/4) по методу центральной разности: $df_central")
println("Точное значение cos(π/4): $(cos(π/4))")
```

### Производные высших порядков

```julia
# Вторая производная
d2f = second_derivative(f, x0)
println("Вторая производная sin(π/4): $d2f (точное значение: $(-sin(π/4)))")

# Производная 3-го порядка
d3f = higher_derivative(f, x0, 3)
println("Третья производная sin(π/4): $d3f (точное значение: $(-cos(π/4)))")
```

### Экстраполяция Ричардсона

```julia
# Улучшение точности с использованием экстраполяции Ричардсона
df_richardson = richardson_extrapolation(f, x0, levels=4)
println("Производная по методу Ричардсона: $df_richardson")

# Сравнение погрешностей
error_central = abs(df_central - cos(π/4))
error_richardson = abs(df_richardson - cos(π/4))
println("Погрешность центрального метода: $error_central")
println("Погрешность метода Ричардсона: $error_richardson")
```

### Адаптивное дифференцирование

```julia
# Адаптивное дифференцирование с контролем точности
result = adaptive_differentiation(f, x0, tol=1e-10)
println("Результат адаптивного дифференцирования: $result")
```

### Унифицированный интерфейс

```julia
# Использование унифицированного интерфейса
df1 = differentiate(f, x0, :forward)
df2 = differentiate(f, x0, :central)
df3 = differentiate(f, x0, :richardson)

println("Производные через унифицированный интерфейс:")
println("- Метод разности вперед: $df1")
println("- Метод центральной разности: $df2")
println("- Метод Ричардсона: $df3")
```

## Структуры и типы данных

### DifferentiationMethod

Перечисление, представляющее различные методы численного дифференцирования:
- `forward`: метод прямой (правой) разности
- `backward`: метод обратной (левой) разности
- `central`: метод центральной разности
- `extrapolation`: метод экстраполяции Ричардсона

### DifferentiationResult

Структура для хранения результатов численного дифференцирования:
- `value`: значение вычисленной производной
- `error_estimate`: оценка погрешности вычисления
- `step_size`: использованный шаг дифференцирования

## Математические основы

### Формулы численного дифференцирования

1. **Метод разности вперед**:
   ```
   f'(x) ≈ [f(x+h) - f(x)] / h + O(h)
   ```

2. **Метод разности назад**:
   ```
   f'(x) ≈ [f(x) - f(x-h)] / h + O(h)
   ```

3. **Метод центральной разности**:
   ```
   f'(x) ≈ [f(x+h) - f(x-h)] / (2h) + O(h²)
   ```

4. **Вторая производная (центральная разность)**:
   ```
   f''(x) ≈ [f(x+h) - 2f(x) + f(x-h)] / h² + O(h²)
   ```

### Метод экстраполяции Ричардсона

Экстраполяция Ричардсона использует комбинацию аппроксимаций с разными шагами для устранения членов погрешности низшего порядка:

```
Improved_f'(x) = [4f'_h/2(x) - f'_h(x)]/3
```

где f'_h обозначает производную, вычисленную с шагом h.

Для метода центральных разностей это дает погрешность порядка O(h⁴), что значительно точнее, чем O(h²).

## Выбор оптимального шага

Выбор оптимального шага h является важным аспектом численного дифференцирования. Существуют два типа ошибок:

1. **Ошибка усечения**: теоретическая ошибка, связанная с аппроксимацией. Уменьшается при уменьшении h.
2. **Ошибка округления**: ошибка из-за конечной точности арифметики компьютера. Увеличивается при уменьшении h.

Оптимальный шаг находится в балансе между этими двумя ошибками:
- Для метода центральных разностей: h_opt ~ ε^(1/3)
- Для методов односторонних разностей: h_opt ~ ε^(1/2)

где ε - машинная точность. 